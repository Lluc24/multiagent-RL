{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!pip install --quiet numpy pandas gymnasium matplotlib pogema seaborn tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import os\n",
    "from tqdm import tqdm\n",
    "from algorithms import JALGT\n",
    "from solution_concepts import MinimaxSolutionConcept, ParetoSolutionConcept, NashSolutionConcept, WelfareSolutionConcept\n",
    "from game_model import GameModel\n",
    "import numpy as np\n",
    "import random\n",
    "from gymnasium import Wrapper\n",
    "from pogema import pogema_v0, GridConfig\n",
    "from pogema.animation import AnimationMonitor, AnimationConfig\n",
    "from utils import draw_history\n",
    "from IPython.display import display, SVG"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Representación de la observación parcial como estado\n",
    "\n",
    "Intentamos que sea lo más compacta posible para que el espacio de estados sea manejable."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def obs_to_state(obs):\n",
    "    matrix_obstacles = obs[0]\n",
    "    matrix_agents = obs[1]\n",
    "    matrix_target = obs[2]\n",
    "\n",
    "    # Representación del objetivo:\n",
    "    #  Ocupa 2 bits\n",
    "    #  0 si el objetivo está arriba, diagonal arriba-izquierda o diagonal arriba-derecha\n",
    "    #  1 si el objetivo está abajo, diagonal abajo-izquierda o diagonal abajo-derecha\n",
    "    #  2 si el objetivo está a la izquierda (no en diagonal)\n",
    "    #  3 si el objetivo está a la derecha (no en diagonal)\n",
    "    target = np.max(matrix_target[2]) * 1 + \\\n",
    "             matrix_target[1][0] * 2 + matrix_target[1][2] * 3\n",
    "\n",
    "    # Representación de los obstáculos:\n",
    "    #  Shift de 2^6, ocupando 4 bits\n",
    "    #  2^9 si hay un obstáculo arriba (no diagonal)\n",
    "    #  2^8 si hay un obstáculo a la izquierda (no diagonal)\n",
    "    #  2^7 si hay un obstáculo a la derecha (no diagonal)\n",
    "    #  2^6 si hay un obstáculo abajo (no diagonal)\n",
    "    obstacles = matrix_obstacles[0][1] * 2 ** 9 + \\\n",
    "                matrix_obstacles[1][0] * 2 ** 8 + \\\n",
    "                matrix_obstacles[1][2] * 2 ** 7 + \\\n",
    "                matrix_obstacles[2][1] * 2 ** 6\n",
    "\n",
    "    # Representación de los otros agentes:\n",
    "    #  Shift de 2^2, ocupando 4 bits\n",
    "    #  2^5 si hay un agente arriba (no diagonal)\n",
    "    #  2^4 si hay un agente a la izquierda (no diagonal)\n",
    "    #  2^3 si hay un agente a la derecha (no diagonal)\n",
    "    #  2^2 si hay un agente abajo (no diagonal)\n",
    "    agents = matrix_agents[0][1] * 2 ** 5 + \\\n",
    "             matrix_agents[1][0] * 2 ** 4 + \\\n",
    "             matrix_agents[1][2] * 2 ** 3 + \\\n",
    "             matrix_agents[2][1] * 2 ** 2\n",
    "\n",
    "    return int(obstacles + agents + target)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Función de recompensa\n",
    "\n",
    "Como hacíamos con FrozenLake, añadimos una penalización por cada paso de tiempo gastado.\n",
    "\n",
    "Tenemos acceso a la observación anterior, por si queremos hacer algo con ella. Por ejemplo, detectar colisiones."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class RewardWrapper(Wrapper):\n",
    "    def __init__(self, env):\n",
    "        super().__init__(env)\n",
    "\n",
    "    def step(self, joint_action):\n",
    "        # En caso de que queráis utilizar las observaciones anteriores, utilizad este objeto:\n",
    "        previous_observations = self.env.unwrapped._obs()\n",
    "\n",
    "        observations, rewards, terminated, truncated, infos = self.env.step(joint_action)\n",
    "        for i in range(len(joint_action)):\n",
    "            if not terminated[i] and not truncated[i]:\n",
    "                if rewards[i] == 0:  # Penalización por tardar más en llegar\n",
    "                    rewards[i] = rewards[i] - 0.01\n",
    "        return observations, rewards, terminated, truncated, infos\n",
    "\n",
    "\n",
    "def create_env(config, seed=42):\n",
    "    grid_config = GridConfig(num_agents=config[\"num_agents\"],\n",
    "                             size=config[\"size\"],\n",
    "                             density=config[\"obstacle_density\"],\n",
    "                             seed=seed,\n",
    "                             max_episode_steps=config[\"episode_length\"],\n",
    "                             obs_radius=1,\n",
    "                             on_target=\"finish\",\n",
    "                             render_mode=None)\n",
    "    animation_config = AnimationConfig(directory='renders/',  # Dónde se guardarán las imágenes\n",
    "                                       static=False,\n",
    "                                       show_agents=True,\n",
    "                                       egocentric_idx=None,  # Punto de vista\n",
    "                                       save_every_idx_episode=config[\"save_every\"],  # Guardar cada save_every episodios\n",
    "                                       show_border=True,\n",
    "                                       show_lines=True)\n",
    "    env = pogema_v0(grid_config)\n",
    "    env = AnimationMonitor(env, animation_config=animation_config)\n",
    "    return RewardWrapper(env)  # Añadimos nuestra función de recompensa"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Configuración del experimento"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "exp_config = {\n",
    "        \"num_agents\": 2,  # Número de agentes\n",
    "        \"size\": 4,  # Tamaño del mapa (valor de anchura y valor de altura)\n",
    "        \"maps\": 10,  # Número de mapas a entrenar y evaluar (se repiten si episodios > mapas)\n",
    "        \"num_states\": 16 * 16 * 4,  # Obstacle representation x Agent representation x Target representation\n",
    "        \"epochs\": 200,  # Cada epoch es un entrenamiento de un número de episodios y una evaluación\n",
    "        \"episodes_per_epoch\": 10,  # Número mínimo de episodios por epoch de entrenamiento\n",
    "        \"episode_length\": 16,  # Número máximo de pasos por episodio, se trunca si se excede\n",
    "        \"obstacle_density\": 0.1,  # Probabilidad de tener un obstáculo en el mapa\n",
    "        \"save_every\": None,  # Frecuencia con que se guarda el SVG con la animación de la ejecución\n",
    "        \"learning_rate\": 0.01,  # alpha\n",
    "        \"epsilon_max\": 1,  # epsilon inicial por epoch\n",
    "        \"epsilon_min\": 0.1,  # cota mínima de epsilon\n",
    "        \"renders\": \"renders/\",  # directorio donde generar las animaciones\n",
    "        \"solution_concept\": ParetoSolutionConcept\n",
    "    }"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Creamos el directorio para los renders:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "try:\n",
    "    os.mkdir(exp_config[\"renders\"])\n",
    "except:\n",
    "    pass"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Entrenamiento y evaluación"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Declaración del modelo de juego y de los algoritmos para cada agente:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "game = GameModel(num_agents=exp_config[\"num_agents\"], num_states=exp_config[\"num_states\"],\n",
    "                 num_actions=5)  # STAY, UP, DOWN, LEFT, RIGHT\n",
    "algorithms = [JALGT(i, game, exp_config[\"solution_concept\"](), epsilon=exp_config[\"epsilon_max\"],\n",
    "                    alpha=exp_config[\"learning_rate\"], seed=i)\n",
    "              for i in range(game.num_agents)]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Variable auxiliar para indicar el decremento de epsilon después de cada episodio. Este decremento es lineal. Se puede hacer un decremento exponencial, modificando el código de manera acorde."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "epsilon_diff = (exp_config[\"epsilon_max\"] - exp_config[\"epsilon_min\"]) / exp_config[\"episodes_per_epoch\"]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Variables auxiliares para almacenar métricas:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "reward_per_epoch = []\n",
    "td_error_per_epoch = []"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Código de entrenamiento y evaluación, bucle sobre epochs:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "pbar = tqdm(range(exp_config[\"epochs\"]))  # Barra de progreso\n",
    "for epoch in pbar:\n",
    "    all_eval_rewards = []\n",
    "    all_td_errors = []\n",
    "\n",
    "    # Entrenamiento\n",
    "    ###############\n",
    "    for ep in range(exp_config[\"episodes_per_epoch\"]):\n",
    "        pbar.set_postfix({'modo': 'entrenamiento', 'episodio': ep})\n",
    "        env = create_env(config=exp_config, seed=ep % exp_config[\"maps\"])\n",
    "        observations, infos = env.reset()\n",
    "        terminated = truncated = [False, ...]\n",
    "        train_rewards = [0] * game.num_agents\n",
    "        states = [obs_to_state(observations[i]) for i in range(game.num_agents)]\n",
    "        while not all(terminated) and not all(truncated):  # Hasta que acabe el episodio\n",
    "            # Elegimos acciones\n",
    "            actions = tuple([algorithms[i].select_action(states[i]) for i in range(game.num_agents)])\n",
    "            # Ejecutamos acciones en el entorno\n",
    "            observations, rewards, terminated, truncated, infos = env.step(actions)\n",
    "            # Aprendemos: actualizamos valores Q\n",
    "            [algorithms[i].learn(actions, rewards, states[i], obs_to_state(observations[i]))\n",
    "             for i in range(game.num_agents)]\n",
    "            # Actualizamos métricas\n",
    "            train_rewards = [train_rewards[i] + rewards[i] for i in range(game.num_agents)]\n",
    "            all_td_errors.append(algorithms[0].metrics[\"td_error\"][-1])\n",
    "            # Preparar siguiente iteración: convertir observaciones parciales en estados\n",
    "            states = [obs_to_state(observations[i]) for i in range(game.num_agents)]\n",
    "        # Actualizamos epsilon\n",
    "        [algorithms[i].set_epsilon(exp_config[\"epsilon_max\"] - epsilon_diff * ep) for i in range(game.num_agents)]\n",
    "    td_error_per_epoch.append(sum(all_td_errors))\n",
    "\n",
    "    # Evaluación\n",
    "    ############\n",
    "    evaluation_episodes = exp_config[\"maps\"]\n",
    "    all_eval_rewards = []\n",
    "    for ep in range(evaluation_episodes):\n",
    "        pbar.set_postfix({'modo': 'evaluación...', 'episodio': ep})\n",
    "        env = create_env(config=exp_config, seed=ep)  # Reaprovechamos mapas del entrenamiento\n",
    "        observations, infos = env.reset()\n",
    "        terminated = truncated = [False, ...]\n",
    "        total_rewards = [0] * exp_config[\"num_agents\"]\n",
    "        states = [obs_to_state(observations[i]) for i in range(game.num_agents)]\n",
    "        while not all(terminated) and not all(truncated):  # Hasta que acabe el episodio\n",
    "            states = [obs_to_state(observations[i]) for i in range(game.num_agents)]\n",
    "            actions = tuple([algorithms[i].select_action(states[i], train=False)\n",
    "                             for i in range(game.num_agents)])\n",
    "            observations, rewards, terminated, truncated, infos = env.step(actions)\n",
    "            total_rewards = [total_rewards[i] + rewards[i] for i in range(exp_config[\"num_agents\"])]\n",
    "        # Guardamos animaciones\n",
    "        for agent_i in range(exp_config[\"num_agents\"]):\n",
    "            solution_concept_name = exp_config[\"solution_concept\"].__name__\n",
    "            env.save_animation(f\"{exp_config['renders']}/{solution_concept_name}-map{ep}-agent{agent_i}-epoch{epoch}.svg\",\n",
    "                               AnimationConfig(egocentric_idx=agent_i, show_border=True, show_lines=True))\n",
    "        all_eval_rewards.append(sum(total_rewards))\n",
    "    pbar.set_description(f\"Recompensa colectiva del último epoch = {'{:>6.6}'.format(str(sum(all_eval_rewards)))}\")\n",
    "    reward_per_epoch.append(sum(all_eval_rewards))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Gráficas con el resultado de la recolección de métricas:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "draw_history(reward_per_epoch, \"Recompensa colectiva\")\n",
    "draw_history(td_error_per_epoch, \"TD Error\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Animación aleatoria:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "animations = list(filter(lambda f: f.endswith(\".svg\"), os.listdir(exp_config[\"renders\"])))\n",
    "random_animation = random.choice(animations)\n",
    "print(random_animation)\n",
    "display(SVG(os.path.join(exp_config[\"renders\"], random_animation)))"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
